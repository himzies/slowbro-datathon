{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j42kQ0k-sJz"
      },
      "source": [
        "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
        "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eRAqjo7z-sJ2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.26.0)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn==1.2.2) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn==1.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn==1.2.2) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\65965\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy scikit-learn==1.2.2\n",
        "# add commented pip installation lines for packages used as shown above for ease of testing\n",
        "# the line should be of the format %pip install PACKAGE_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ml5cLgz-sJ3"
      },
      "source": [
        "## **DO NOT CHANGE** the filepath variable\n",
        "##### Instead, create a folder named 'data' in your current working directory and\n",
        "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "bSuWvnQF-sJ4"
      },
      "outputs": [],
      "source": [
        "# Can have as many cells as you want for code\n",
        "import pandas as pd\n",
        "import warnings\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import numpy as np\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "filepath = \"./data/catB_train.parquet\"\n",
        "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLRyM6NJ-sJ4"
      },
      "source": [
        "### **ALL** Code for machine learning and dataset analysis should be entered below.\n",
        "##### Ensure that your code is clear and readable.\n",
        "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_udFbhBw-sJ5"
      },
      "outputs": [],
      "source": [
        "# read in data\n",
        "df = pd.read_parquet(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "x3a50vBq3tD4"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df):\n",
        "    # impute empty values for race and country\n",
        "    df[['race_desc', 'ctrycode_desc']] = df[['race_desc', 'ctrycode_desc']].fillna(value='Others')\n",
        "\n",
        "    # convert dates to number of years\n",
        "    rownum = len(df.index)\n",
        "    today_date = [datetime.today()] * rownum\n",
        "    df['today'] = today_date\n",
        "    df['min_occ_date'].replace('None', '1900-01-01', inplace=True) # set to a very high number\n",
        "    df['cltdob_fix'].replace('None', '1900-01-01', inplace=True)\n",
        "    df['min_occ_date']= pd.to_datetime(df['min_occ_date'])\n",
        "    df['cltdob_fix']= pd.to_datetime(df['cltdob_fix'])\n",
        "    col_index_1 = df.columns.get_loc('min_occ_date')\n",
        "    col_index_2 = df.columns.get_loc('cltdob_fix')\n",
        "    df.insert(loc=col_index_1, column='years_since_last_bought', value=(df['min_occ_date'] - df['today']) / np.timedelta64(365, '1D'))\n",
        "    df.insert(loc=col_index_2, column='customer_age', value=(df['cltdob_fix'] - df['today']) / np.timedelta64(365, '1D'))\n",
        "    df['years_since_last_bought'] = df['years_since_last_bought'].round().abs()\n",
        "    df['customer_age'] = df['customer_age'].round().abs()\n",
        "\n",
        "    # drop irrelevant columns\n",
        "    df = df.drop(['clntnum', 'today', 'min_occ_date', 'cltdob_fix'], axis=1)\n",
        "\n",
        "    # impute -1, 0, 9999 for NA values in lapse_ape, n_months_since, flg_gi_claim, clmcon, hlthclaim and giclaim columns\n",
        "    lapse_ape_colnames = list(df.filter(regex=\"lapse_ape\").columns)\n",
        "    n_months_since_colnames = list(df.filter(regex=\"n_months_since\").columns)\n",
        "    flg_gi_claim_colnames = list(df.filter(regex=\"flg_gi_claim\").columns)\n",
        "    clmcon_colnames = list(df.filter(regex=\"clmcon\").columns)\n",
        "    hlthclaim_colnames = list(df.filter(regex=\"hlthclaim\").columns)\n",
        "    giclaim_colnames = list(df.filter(regex=\"giclaim\").columns)\n",
        "\n",
        "    df[lapse_ape_colnames] = df[lapse_ape_colnames].fillna(value=0)\n",
        "    df[n_months_since_colnames] = df[n_months_since_colnames].fillna(value=9999)\n",
        "    df[flg_gi_claim_colnames] = df[flg_gi_claim_colnames].fillna(value=0)\n",
        "    df[clmcon_colnames] = df[clmcon_colnames].fillna(value=-1)\n",
        "    df[hlthclaim_colnames] = df[hlthclaim_colnames].fillna(value=-1)\n",
        "    df[giclaim_colnames] = df[giclaim_colnames].fillna(value=-1)\n",
        "\n",
        "    # impute 0 for these columns\n",
        "    columns_to_impute = ['tot_cancel_pols', 'recency_lapse', 'recency_cancel', 'flg_affconnect_show_interest_ever', 'flg_affconnect_ready_to_buy_ever', 'affcon_visit_days']\n",
        "    df[columns_to_impute] = df[columns_to_impute].fillna(value=0)\n",
        "\n",
        "    # impute -1 for these flg_affconnect_lapse_ever columns\n",
        "    df['flg_affconnect_lapse_ever'] = df['flg_affconnect_lapse_ever'].fillna(value=-1)\n",
        "\n",
        "    # drop columns with only one value\n",
        "    nunique = df.nunique()\n",
        "    cols_to_drop = nunique[nunique == 1].index\n",
        "    df = df.drop(cols_to_drop, axis=1)\n",
        "\n",
        "    # split dataset into 'P' group and 'C' or 'G' group\n",
        "    df_p = df[df['clttype'] == 'P']\n",
        "    df_cg = df[(df['clttype'] == 'C') | (df['clttype'] == 'G')]\n",
        "\n",
        "    # 'P' group specific feature engineering\n",
        "    # drop specific rows with na values\n",
        "    df_p = df_p.dropna(subset=['hh_size', 'annual_income_est'])\n",
        "    n_months_colnames = list(df_p.filter(regex=\"n_months\").columns)\n",
        "    for col in n_months_colnames:\n",
        "      df_p[col] = df_p[col].astype(int)\n",
        "      df_p.drop(df_p[df_p[col] < 0].index, inplace = True)\n",
        "\n",
        "    # drop hh_20, pop_20, hh_size_est column, clttype and round hh_size column\n",
        "    df_p = df_p.drop(['hh_20', 'pop_20', 'hh_size_est', 'clttype'], axis=1)\n",
        "\n",
        "    # round hh_size column\n",
        "    df_p['hh_size'] = df_p['hh_size'].round()\n",
        "\n",
        "    # 'C' or 'G' group specific feature engineering\n",
        "    # drop specific rows with -ve values\n",
        "    n_months_colnames = list(df_cg.filter(regex=\"n_months\").columns)\n",
        "    for col in n_months_colnames:\n",
        "      df_cg[col] = df_cg[col].astype(int)\n",
        "      df_cg.drop(df_cg[df_cg[col] < 0].index, inplace = True)\n",
        "\n",
        "    # drop hh_20, pop_20, hh_size_est column, hh_size, annual_income_est, race_desc and round hh_size column\n",
        "    df_cg = df_cg.drop(['hh_20', 'pop_20', 'hh_size_est', 'hh_size', 'annual_income_est', 'race_desc'], axis=1)\n",
        "\n",
        "    # fill remaining rows with mode\n",
        "    for column in df_p.columns:\n",
        "      df_p[column].fillna(df_p[column].mode()[0], inplace=True)\n",
        "    df_p.reset_index()\n",
        "\n",
        "    for column in df_cg.columns:\n",
        "      df_cg[column].fillna(df_cg[column].mode()[0], inplace=True)\n",
        "    df_cg.reset_index()\n",
        "\n",
        "    # one-hot encoding of categorical variables\n",
        "    df_p_ohe = pd.get_dummies(df_p, columns = ['stat_flag', 'race_desc', 'ctrycode_desc', 'cltsex_fix', 'annual_income_est'])\n",
        "    df_cg_ohe = pd.get_dummies(df_cg, columns = ['stat_flag', 'clttype', 'ctrycode_desc', 'cltsex_fix'])\n",
        "\n",
        "    # drop columns from xgboost feature engineering\n",
        "    df_p_ohe = df_p_ohe[['is_valid_dm', 'is_valid_email', 'is_class_1_2', 'n_months_last_bought_gi', 'f_mindef_mha', 'f_retail']]\n",
        "    df_cg_ohe = df_cg_ohe[['years_since_last_bought', 'customer_age', 'flg_substandard', 'flg_is_borderline_standard',\n",
        "                           'flg_is_revised_term', 'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim',\n",
        "                           'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation', 'flg_is_returned_mail', 'is_consent_to_mail',\n",
        "                           'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email',\n",
        "                           'is_housewife_retiree', 'is_sg_pr', 'is_class_1_2', 'n_months_last_bought_products', 'flg_latest_being_lapse',\n",
        "                           'flg_latest_being_cancel', 'recency_lapse', 'recency_cancel', 'tot_inforce_pols', 'tot_cancel_pols',\n",
        "                           'ape_grp_6fc3e6', 'ape_grp_de05ae', 'ape_grp_945b5a', 'ape_grp_6a5788', 'ape_ltc_43b9d5', 'ape_grp_9cdedf',\n",
        "                           'ape_grp_1581d7', 'ape_grp_22decf', 'ape_lh_507c37', 'ape_lh_839f8a', 'ape_inv_e9f316', 'ape_grp_caa6ff',\n",
        "                           'ape_grp_fd3bfb', 'ape_lh_e22a6a', 'ape_grp_70e1dd', 'ape_grp_e04c3a', 'ape_grp_fe5fb8', 'ape_grp_94baec',\n",
        "                           'ape_grp_e91421', 'ape_lh_f852af', 'ape_lh_947b15', 'sumins_grp_6fc3e6', 'sumins_grp_de05ae', 'sumins_grp_945b5a',\n",
        "                           'sumins_grp_6a5788', 'sumins_ltc_43b9d5', 'sumins_grp_9cdedf', 'sumins_grp_1581d7', 'sumins_lh_507c37', 'sumins_inv_e9f316',\n",
        "                           'sumins_grp_caa6ff', 'sumins_grp_fd3bfb', 'sumins_grp_70e1dd', 'sumins_grp_fe5fb8', 'sumins_grp_e91421', 'sumins_lh_f852af',\n",
        "                           'sumins_lh_947b15', 'prempaid_grp_6fc3e6', 'prempaid_grp_de05ae', 'prempaid_grp_945b5a', 'prempaid_grp_6a5788',\n",
        "                           'prempaid_ltc_43b9d5', 'prempaid_grp_9cdedf', 'prempaid_grp_1581d7', 'prempaid_grp_22decf', 'prempaid_lh_507c37',\n",
        "                           'prempaid_lh_839f8a', 'prempaid_inv_e9f316', 'prempaid_grp_caa6ff', 'prempaid_grp_fd3bfb', 'prempaid_lh_e22a6a',\n",
        "                           'prempaid_grp_70e1dd', 'prempaid_grp_e04c3a', 'prempaid_grp_fe5fb8', 'prempaid_grp_94baec', 'prempaid_grp_e91421',\n",
        "                           'prempaid_lh_f852af', 'prempaid_lh_947b15', 'ape_839f8a', 'ape_e22a6a', 'ape_c4bda5', 'ape_ltc', 'ape_507c37',\n",
        "                           'f_hold_839f8a', 'f_hold_e22a6a', 'f_hold_c4bda5', 'f_hold_ltc', 'f_hold_507c37', 'sumins_839f8a', 'sumins_c4bda5',\n",
        "                           'sumins_ltc', 'sumins_507c37', 'prempaid_839f8a', 'prempaid_e22a6a', 'prempaid_c4bda5', 'prempaid_ltc', 'prempaid_507c37',\n",
        "                           'lapse_ape_grp_6fc3e6', 'lapse_ape_grp_de05ae', 'lapse_ape_grp_945b5a', 'lapse_ape_grp_6a5788', 'lapse_ape_ltc_43b9d5',\n",
        "                           'lapse_ape_grp_9cdedf', 'lapse_ape_grp_1581d7', 'lapse_ape_grp_22decf', 'lapse_ape_lh_507c37', 'lapse_ape_lh_839f8a',\n",
        "                           'lapse_ape_inv_e9f316', 'lapse_ape_grp_caa6ff', 'lapse_ape_grp_fd3bfb', 'lapse_ape_lh_e22a6a', 'lapse_ape_grp_70e1dd',\n",
        "                           'lapse_ape_grp_e04c3a', 'lapse_ape_grp_fe5fb8', 'lapse_ape_grp_94baec', 'lapse_ape_grp_e91421', 'lapse_ape_lh_f852af',\n",
        "                           'lapse_ape_lh_947b15', 'n_months_since_lapse_grp_6fc3e6', 'n_months_since_lapse_grp_de05ae', 'n_months_since_lapse_grp_945b5a',\n",
        "                           'n_months_since_lapse_grp_6a5788', 'n_months_since_lapse_ltc_43b9d5', 'n_months_since_lapse_grp_9cdedf',\n",
        "                           'n_months_since_lapse_grp_1581d7', 'n_months_since_lapse_grp_22decf', 'n_months_since_lapse_lh_507c37',\n",
        "                           'n_months_since_lapse_lh_839f8a', 'n_months_since_lapse_inv_e9f316', 'n_months_since_lapse_grp_caa6ff',\n",
        "                           'n_months_since_lapse_grp_fd3bfb', 'n_months_since_lapse_lh_e22a6a', 'n_months_since_lapse_grp_70e1dd',\n",
        "                           'n_months_since_lapse_grp_e04c3a', 'n_months_since_lapse_grp_fe5fb8', 'n_months_since_lapse_grp_94baec',\n",
        "                           'n_months_since_lapse_grp_e91421', 'n_months_since_lapse_lh_f852af', 'n_months_since_lapse_lh_947b15',\n",
        "                           'f_ever_bought_839f8a', 'f_ever_bought_e22a6a', 'f_ever_bought_c4bda5', 'f_ever_bought_ltc', 'f_ever_bought_507c37',\n",
        "                           'f_ever_bought_gi', 'n_months_last_bought_839f8a', 'n_months_last_bought_e22a6a', 'n_months_last_bought_c4bda5',\n",
        "                           'n_months_last_bought_ltc', 'n_months_last_bought_507c37', 'n_months_last_bought_gi', 'f_ever_bought_grp_6fc3e6',\n",
        "                           'f_ever_bought_grp_de05ae', 'f_ever_bought_grp_945b5a', 'f_ever_bought_grp_6a5788', 'f_ever_bought_ltc_43b9d5',\n",
        "                           'f_ever_bought_grp_9cdedf', 'f_ever_bought_grp_1581d7', 'f_ever_bought_grp_22decf', 'f_ever_bought_lh_507c37',\n",
        "                           'f_ever_bought_lh_839f8a', 'f_ever_bought_inv_e9f316', 'f_ever_bought_grp_caa6ff', 'f_ever_bought_grp_fd3bfb',\n",
        "                           'f_ever_bought_lh_e22a6a', 'f_ever_bought_grp_70e1dd', 'f_ever_bought_grp_e04c3a', 'f_ever_bought_grp_fe5fb8',\n",
        "                           'f_ever_bought_grp_94baec', 'f_ever_bought_grp_e91421', 'f_ever_bought_lh_f852af', 'f_ever_bought_lh_947b15',\n",
        "                           'n_months_last_bought_grp_6fc3e6', 'n_months_last_bought_grp_de05ae', 'n_months_last_bought_grp_945b5a',\n",
        "                           'n_months_last_bought_grp_6a5788', 'n_months_last_bought_ltc_43b9d5', 'n_months_last_bought_grp_9cdedf',\n",
        "                           'n_months_last_bought_grp_1581d7', 'n_months_last_bought_grp_22decf', 'n_months_last_bought_lh_507c37',\n",
        "                           'n_months_last_bought_lh_839f8a', 'n_months_last_bought_inv_e9f316', 'n_months_last_bought_grp_caa6ff',\n",
        "                           'n_months_last_bought_grp_fd3bfb', 'n_months_last_bought_lh_e22a6a', 'n_months_last_bought_grp_70e1dd',\n",
        "                           'n_months_last_bought_grp_e04c3a', 'n_months_last_bought_grp_fe5fb8', 'n_months_last_bought_grp_94baec',\n",
        "                           'n_months_last_bought_grp_e91421', 'n_months_last_bought_lh_f852af', 'n_months_last_bought_lh_947b15',\n",
        "                           'n_months_last_bought_32c74c', 'f_elx', 'f_mindef_mha', 'f_retail', 'flg_affconnect_show_interest_ever',\n",
        "                           'flg_affconnect_ready_to_buy_ever', 'flg_affconnect_lapse_ever', 'affcon_visit_days', 'n_months_since_visit_affcon',\n",
        "                           'clmcon_visit_days', 'recency_clmcon', 'recency_clmcon_regis', 'hlthclaim_amt', 'recency_hlthclaim',\n",
        "                           'hlthclaim_cnt_success', 'recency_hlthclaim_success', 'hlthclaim_cnt_unsuccess', 'recency_hlthclaim_unsuccess',\n",
        "                           'flg_hlthclaim_839f8a_ever', 'recency_hlthclaim_839f8a', 'flg_hlthclaim_14cb37_ever', 'recency_hlthclaim_14cb37',\n",
        "                           'giclaim_amt', 'recency_giclaim', 'stat_flag_ACTIVE', 'stat_flag_LAPSED', 'clttype_C', 'clttype_G',\n",
        "                           'ctrycode_desc_Others', 'ctrycode_desc_Singapore', 'cltsex_fix_Female', 'cltsex_fix_Male']]\n",
        "\n",
        "    return df_p_ohe, df_cg_ohe\n",
        "\n",
        "def load_model():\n",
        "    filepath_p = \"./models/rf_model_p.pkl\"\n",
        "    filepath_cg = \"./models/rf_model_cg.pkl\"\n",
        "\n",
        "    # load model from pickle file\n",
        "    with open(filepath_p, 'rb') as file:\n",
        "        model_p = pickle.load(file)\n",
        "\n",
        "    with open(filepath_cg, 'rb') as file:\n",
        "        model_cg = pickle.load(file)\n",
        "    \n",
        "    return model_p, model_cg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzXaby59-sJ5"
      },
      "source": [
        "## The cell below is **NOT** to be removed\n",
        "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list).\n",
        "##### It is recommended to test the function out prior to submission\n",
        "-------------------------------------------------------------------------------------------------------------------------------\n",
        "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
        "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "9mXKZ_KL-sJ6"
      },
      "outputs": [],
      "source": [
        "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
        "    '''DO NOT REMOVE THIS FUNCTION.\n",
        "\n",
        "The function accepts a dataframe as input and return an iterable (list)\n",
        "of binary classes as output.\n",
        "\n",
        "The function should be coded to test on hidden data\n",
        "and should include any preprocessing functions needed for your model to perform.\n",
        "\n",
        "All relevant code MUST be included in this function.'''\n",
        "\n",
        "    # apply feature engineering to data\n",
        "    df_p, df_cg = feature_engineering(hidden_data)\n",
        "    X_test_p = df_p\n",
        "    X_test_cg = df_cg\n",
        "\n",
        "    # load model\n",
        "    model_p, model_cg = load_model()\n",
        "\n",
        "    # make predictions\n",
        "    y_pred_p = model_p.predict(X_test_p)\n",
        "    y_pred_cg = model_cg.predict(X_test_cg)\n",
        "\n",
        "    # combine the two predictions\n",
        "    p_pred_ind = pd.DataFrame({'index': df_p.index, 'y_pred': y_pred_p})\n",
        "    cg_pred_ind = pd.DataFrame({'index': df_cg.index, 'y_pred': y_pred_cg})\n",
        "    combined_pred_ind = pd.concat([p_pred_ind, cg_pred_ind])\n",
        "    orig_ind = pd.DataFrame({'index': hidden_data.index})\n",
        "    orig_pred = pd.merge(orig_ind, combined_pred_ind, on='index', how='left')\n",
        "    orig_pred = orig_pred.sort_values(by=['index'])\n",
        "    orig_pred = orig_pred.set_index('index')\n",
        "    orig_pred = orig_pred.fillna(0)\n",
        "\n",
        "    # convert pandas series to list\n",
        "    result = orig_pred['y_pred'].tolist()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFPCWiVp-sJ6"
      },
      "source": [
        "##### Cell to check testing_hidden_data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "j7j16x0Z-sJ7"
      },
      "outputs": [],
      "source": [
        "# This cell should output a list of predictions.\n",
        "test_df = pd.read_parquet(filepath)\n",
        "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
        "print(testing_hidden_data(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Ox2Wfo-sJ7"
      },
      "source": [
        "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
